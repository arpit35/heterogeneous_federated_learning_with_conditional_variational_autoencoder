{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44863bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arpit\\AppData\\Roaming\\Python\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2026-01-05 03:37:44,407\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from src.ml_models.utils import count_params\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from src.ml_models.utils import get_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8f8d2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_path = \"model_architecture/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ae61c2",
   "metadata": {},
   "source": [
    "## configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76775beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_architecture = False\n",
    "show_model_vram_usage = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34d24b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vram_usage(training_function):\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "    training_function()\n",
    "\n",
    "    current = torch.cuda.memory_allocated() / 1024**2\n",
    "    peak = torch.cuda.max_memory_allocated() / 1024**2\n",
    "\n",
    "    print(f\"Current VRAM: {current:.2f} MB\")\n",
    "    print(f\"Peak VRAM: {peak:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "257bc288",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader():\n",
    "\n",
    "    class InMemoryDictDataset(Dataset):\n",
    "        def __init__(self, num_samples, num_classes=10):\n",
    "            self.images = torch.rand(num_samples, 1, 28, 28)\n",
    "            self.labels = torch.randint(\n",
    "                0, num_classes, (num_samples,), dtype=torch.long\n",
    "            )\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.images)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            return {\n",
    "                \"image\": self.images[idx],\n",
    "                \"label\": self.labels[idx],\n",
    "            }\n",
    "\n",
    "    dataset = InMemoryDictDataset(num_samples=10000)\n",
    "\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=64,\n",
    "        shuffle=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c15592",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "246a03eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current VRAM: 67.14 MB\n",
      "Peak VRAM: 155.09 MB\n",
      "Cnn Parameters: 6669258\n"
     ]
    }
   ],
   "source": [
    "from src.ml_models.cnn import CNN\n",
    "from src.ml_models.train_net import train_net\n",
    "\n",
    "cnn = CNN(\"cnn5\")\n",
    "\n",
    "if show_model_vram_usage or True:\n",
    "    dataloader = get_dataloader()\n",
    "\n",
    "    vram_usage(\n",
    "        lambda: train_net(\n",
    "            net=cnn,\n",
    "            trainloader=dataloader,\n",
    "            testloader=dataloader,\n",
    "            epochs=10,\n",
    "            learning_rate=0.001,\n",
    "            device=get_device(),\n",
    "            dataset_input_feature=\"image\",\n",
    "            dataset_target_feature=\"label\",\n",
    "            optimizer_strategy=\"adam\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    cnn.to(\"cpu\")\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "if save_model_architecture:\n",
    "    cnn_dummy_input = torch.randn(1, 3, 32, 32)\n",
    "    torch.onnx.export(cnn, cnn_dummy_input, save_model_path + \"cnn.onnx\")\n",
    "\n",
    "print(\"Cnn Parameters:\", count_params(cnn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee8f86a",
   "metadata": {},
   "source": [
    "# HFedCVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d782691f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current VRAM: 63.85 MB\n",
      "Peak VRAM: 157.03 MB\n",
      "Vae Parameters: 5916921\n"
     ]
    }
   ],
   "source": [
    "from src.ml_models.vae import CVAE\n",
    "from src.ml_models.train_vae import train_vae\n",
    "\n",
    "HFedCVAE = {\n",
    "    \"cvae_parameters\": {\n",
    "        \"h_dim\": 224,\n",
    "        \"res_h_dim\": 56,\n",
    "        \"n_res_layers\": 3,\n",
    "        \"latent_dim\": 100,\n",
    "    }\n",
    "}\n",
    "\n",
    "cvae = CVAE(**HFedCVAE[\"cvae_parameters\"])\n",
    "\n",
    "if show_model_vram_usage or True:\n",
    "    dataloader = get_dataloader()\n",
    "\n",
    "    vram_usage(\n",
    "        lambda: train_vae(\n",
    "            cvae=cvae,\n",
    "            trainloader=dataloader,\n",
    "            epochs=10,\n",
    "            device=get_device(),\n",
    "            dataset_input_feature=\"image\",\n",
    "            dataset_target_feature=\"label\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    cvae.to(\"cpu\")\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "if save_model_architecture:\n",
    "    cvae_dummy_input = torch.randn(1, 1, 28, 28)\n",
    "    torch.onnx.export(cvae, cvae_dummy_input, save_model_path + \"HFedCVAE_vae.onnx\")\n",
    "\n",
    "print(\"Vae Parameters:\", count_params(cvae))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e9dc18",
   "metadata": {},
   "source": [
    "# HFedCGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd857fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current VRAM: 26.57 MB\n",
      "Peak VRAM: 151.22 MB\n",
      "Generator Parameters: 675421\n",
      "Discriminator Parameters: 674353\n",
      "Total GAN Parameters: 1349774\n"
     ]
    }
   ],
   "source": [
    "from src.ml_models.train_gan import train_gan\n",
    "from src.ml_models.discriminator import Discriminator\n",
    "from src.ml_models.generator import Generator\n",
    "\n",
    "HFedCGAN = {\n",
    "    \"generator_parameters\": {\n",
    "        \"n_block_layers\": 2,\n",
    "        \"h_dim\": 120,\n",
    "        \"latent_dim\": 100,\n",
    "        \"init_img_dim\": 7,\n",
    "    },\n",
    "    \"discriminator_parameters\": {\n",
    "        \"block_repeat\": 1,\n",
    "        \"n_block_layers\": 3,\n",
    "        \"h_dim\": 42,\n",
    "    },\n",
    "}\n",
    "\n",
    "generator = Generator(**HFedCGAN[\"generator_parameters\"])\n",
    "discriminator = Discriminator(**HFedCGAN[\"discriminator_parameters\"])\n",
    "\n",
    "if show_model_vram_usage or True:\n",
    "    dataloader = get_dataloader()\n",
    "\n",
    "    vram_usage(\n",
    "        lambda: train_gan(\n",
    "            generator=generator,\n",
    "            discriminator=discriminator,\n",
    "            trainloader=dataloader,\n",
    "            epochs=10,\n",
    "            device=get_device(),\n",
    "            dataset_input_feature=\"image\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    generator.to(\"cpu\")\n",
    "    discriminator.to(\"cpu\")\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "if save_model_architecture:\n",
    "    generator_dummy_input = torch.randn(1, 100)\n",
    "    torch.onnx.export(\n",
    "        generator, generator_dummy_input, save_model_path + \"HFedCGAN_generator.onnx\"\n",
    "    )\n",
    "    discriminator_dummy_input = torch.randn(1, 1, 28, 28)\n",
    "    torch.onnx.export(\n",
    "        discriminator,\n",
    "        discriminator_dummy_input,\n",
    "        save_model_path + \"HFedCGAN_discriminator.onnx\",\n",
    "    )\n",
    "\n",
    "generator_parameters_count = count_params(generator)\n",
    "discriminator_parameters_count = count_params(discriminator)\n",
    "\n",
    "print(\"Generator Parameters:\", generator_parameters_count)\n",
    "print(\"Discriminator Parameters:\", discriminator_parameters_count)\n",
    "print(\n",
    "    \"Total GAN Parameters:\",\n",
    "    generator_parameters_count + discriminator_parameters_count,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5d588c",
   "metadata": {},
   "source": [
    "# HFedCVAEGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0835a572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current VRAM: 28.94 MB\n",
      "Peak VRAM: 103.82 MB\n",
      "Vae Parameters: 1808301\n",
      "Discriminator Parameters: 345601\n",
      "Total VAE-GAN Parameters: 2153902\n"
     ]
    }
   ],
   "source": [
    "from src.ml_models.vae import VAE\n",
    "from src.ml_models.discriminator import Discriminator\n",
    "from src.ml_models.train_vae_gan import train_vae_gan\n",
    "\n",
    "HFedCVAEGAN = {\n",
    "    \"vae_parameters\": {\n",
    "        \"h_dim\": 210,\n",
    "        \"res_h_dim\": 56,\n",
    "        \"n_res_layers\": 2,\n",
    "        \"latent_dim\": 100,\n",
    "    },\n",
    "    \"discriminator_parameters\": {\n",
    "        \"block_repeat\": 1,\n",
    "        \"n_block_layers\": 3,\n",
    "        \"h_dim\": 30,\n",
    "    },\n",
    "}\n",
    "\n",
    "vae = VAE(**HFedCVAEGAN[\"vae_parameters\"])\n",
    "discriminator = Discriminator(**HFedCVAEGAN[\"discriminator_parameters\"])\n",
    "\n",
    "if show_model_vram_usage or True:\n",
    "    dataloader = get_dataloader()\n",
    "\n",
    "    vram_usage(\n",
    "        lambda: train_vae_gan(\n",
    "            vae=vae,\n",
    "            discriminator=discriminator,\n",
    "            trainloader=dataloader,\n",
    "            epochs=10,\n",
    "            device=get_device(),\n",
    "            dataset_input_feature=\"image\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    vae.to(\"cpu\")\n",
    "    discriminator.to(\"cpu\")\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "if save_model_architecture:\n",
    "    vae_dummy_input = torch.randn(1, 1, 28, 28)\n",
    "    torch.onnx.export(vae, vae_dummy_input, save_model_path + \"HFedCVAEGAN_vae.onnx\")\n",
    "    discriminator_dummy_input = torch.randn(1, 1, 28, 28)\n",
    "    torch.onnx.export(\n",
    "        discriminator,\n",
    "        discriminator_dummy_input,\n",
    "        save_model_path + \"HFedCVAEGAN_discriminator.onnx\",\n",
    "    )\n",
    "\n",
    "vae_parameters_count = count_params(vae)\n",
    "discriminator_parameters_count = count_params(discriminator)\n",
    "\n",
    "print(\"Vae Parameters:\", vae_parameters_count)\n",
    "print(\"Discriminator Parameters:\", discriminator_parameters_count)\n",
    "print(\n",
    "    \"Total VAE-GAN Parameters:\",\n",
    "    vae_parameters_count + discriminator_parameters_count,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9b6c9f",
   "metadata": {},
   "source": [
    "# Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "429edf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.scripts.helper import save_metadata\n",
    "\n",
    "save_metadata(\n",
    "    {\n",
    "        \"HFedCVAE\": HFedCVAE,\n",
    "        \"HFedCGAN\": HFedCGAN,\n",
    "        \"HFedCVAEGAN\": HFedCVAEGAN,\n",
    "    }\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
